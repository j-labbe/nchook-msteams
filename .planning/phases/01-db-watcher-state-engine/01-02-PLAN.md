---
phase: 01-db-watcher-state-engine
plan: 02
type: execute
wave: 2
depends_on: ["01-01"]
files_modified:
  - nchook.py
autonomous: false

must_haves:
  truths:
    - "Daemon detects new notifications within seconds via kqueue on WAL file"
    - "Daemon falls back to periodic polling when kqueue events are missed"
    - "Daemon re-registers kqueue when WAL file is deleted/renamed during checkpoint"
    - "Killing and restarting the daemon resumes from the last processed notification"
    - "Daemon prints extracted fields (app, title, subtitle, body, timestamp) for each new notification"
  artifacts:
    - path: "nchook.py"
      provides: "kqueue event loop with fallback polling"
      contains: "def run_watcher"
    - path: "nchook.py"
      provides: "CLI entry point"
      contains: "def main"
    - path: "nchook.py"
      provides: "kqueue WAL watcher setup"
      contains: "KQ_FILTER_VNODE"
  key_links:
    - from: "run_watcher"
      to: "query_new_notifications"
      via: "called on each kqueue event or poll timeout"
      pattern: "query_new_notifications\\(conn"
    - from: "run_watcher"
      to: "save_state"
      via: "called after processing each batch"
      pattern: "save_state\\(last_rec_id"
    - from: "run_watcher"
      to: "select.kqueue"
      via: "kqueue event loop with KQ_FILTER_VNODE on WAL fd"
      pattern: "kq\\.control"
    - from: "main"
      to: "validate_environment"
      via: "startup sequence before entering event loop"
      pattern: "validate_environment"
    - from: "main"
      to: "run_watcher"
      via: "enters event loop after startup"
      pattern: "run_watcher"
---

<objective>
Wire together all Plan 01 functions into a working kqueue event loop and CLI entry point, making nchook.py a complete runnable daemon.

Purpose: Plan 01 built the individual engine components (DB access, plist parsing, state persistence). This plan composes them into the actual daemon: a kqueue-driven event loop that watches the WAL file for changes, queries for new notifications, logs extracted fields, persists state, and handles WAL file recreation gracefully. The result is the complete Phase 1 deliverable.

Output: A runnable `python3 nchook.py` that watches the notification DB, detects new notifications in near-real-time, prints extracted fields, and persists its read position.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-db-watcher-state-engine/01-RESEARCH.md
@.planning/phases/01-db-watcher-state-engine/01-01-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement kqueue event loop and main entry point</name>
  <files>nchook.py</files>
  <action>
Add the event loop and CLI entry point to `nchook.py` (which already has all foundation functions from Plan 01).

**1. Constants:**
- `POLL_FALLBACK_SECONDS = 5.0` -- fallback poll interval when kqueue misses events (WAL checkpoint edge case)

**2. `create_wal_watcher(wal_path)` function:**
- Create `select.kqueue()` instance
- Open WAL file read-only: `os.open(wal_path, os.O_RDONLY)`
- Create kevent with: `filter=select.KQ_FILTER_VNODE`, `flags=KQ_EV_ADD | KQ_EV_ENABLE | KQ_EV_CLEAR`, `fflags=KQ_NOTE_WRITE | KQ_NOTE_DELETE | KQ_NOTE_RENAME`
- Return `(kq, fd, kevent)` tuple

**3. `run_watcher(db_path, wal_path, state_path)` function:**
- Load state: `last_rec_id = load_state(state_path)`
- Open DB: `conn = sqlite3.connect(f"file:{db_path}?mode=ro", uri=True, timeout=5.0)` with `row_factory = sqlite3.Row`
- Check DB consistency: `last_rec_id = check_db_consistency(conn, last_rec_id)`
- Print startup summary
- Set up kqueue if WAL exists, otherwise log warning and use poll-only mode
- Main loop (`while running`):
  - If kqueue: `events = kq.control([kev], 1, POLL_FALLBACK_SECONDS)`. Catch `OSError` (stale FD) and log warning.
  - If no kqueue: `time.sleep(POLL_FALLBACK_SECONDS)`
  - Check for WAL delete/rename events (`ev.fflags & (KQ_NOTE_DELETE | KQ_NOTE_RENAME)`)
  - Call `query_new_notifications(conn, last_rec_id)`
  - For each notification: log with `logging.info` showing app, title, subtitle, body, timestamp (formatted as ISO 8601)
  - Update `last_rec_id` to highest rec_id in batch
  - If any new notifications found: `save_state(last_rec_id, state_path)`
  - If WAL was deleted/renamed AND WAL file exists again: close old FD, re-open, re-register kqueue event. Log "Re-registered kqueue on new WAL file".
- finally block: close WAL FD, close kqueue, close DB connection
- Use module-level `running = True` flag (will be used by signal handlers in Phase 3, but for now the loop runs until Ctrl+C/KeyboardInterrupt)

**4. `main()` function:**
- Set up logging: `logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s', datefmt='%Y-%m-%d %H:%M:%S')`
- Call `detect_db_path()` to get `(db_path, wal_path)`
- Call `validate_environment(db_path)` -- this exits on failure
- Close the validation connection (validate_environment returns a connection, but run_watcher opens its own)
- Wrap `run_watcher(db_path, wal_path, STATE_FILE)` in try/except `KeyboardInterrupt`: log "Shutting down..." and exit cleanly

**5. `if __name__ == "__main__": main()` guard at bottom.**

**Important details:**
- Do NOT add argparse or CLI flags yet (Phase 3 adds --dry-run)
- The `running` flag should be a module-level variable so Phase 3 can set it False from a signal handler
- When logging notifications, format timestamp as ISO 8601: `time.strftime('%Y-%m-%dT%H:%M:%S', time.gmtime(notif['timestamp']))` if timestamp > 0, else "unknown"
- Handle `KeyboardInterrupt` in main() to exit gracefully without traceback
  </action>
  <verify>
1. Run `python3 -c "from nchook import run_watcher, main, create_wal_watcher; print('event loop functions exist')"` -- should print without error.

2. Run `python3 -c "
import nchook
import inspect
src = inspect.getsource(nchook.run_watcher)
assert 'kq.control' in src or 'kqueue' in src, 'Missing kqueue usage'
assert 'query_new_notifications' in src, 'Missing DB query call'
assert 'save_state' in src, 'Missing state persistence'
assert 'KQ_NOTE_DELETE' in src or 'NOTE_DELETE' in src, 'Missing WAL delete handling'
print('Event loop wiring verified')
"` -- should confirm all key connections exist in the source.

3. Run `python3 nchook.py 2>&1 | head -5` -- should either:
   - Print the startup summary (if FDA is granted), OR
   - Print the FDA error message (if FDA is not granted)
   Either outcome confirms the startup sequence works.
  </verify>
  <done>
nchook.py is a complete, runnable daemon. `python3 nchook.py` starts the kqueue event loop (or prints FDA error if not granted). The loop queries for new notifications on WAL changes or every 5s fallback, logs extracted fields, persists state, and handles WAL recreation.
  </done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <name>Task 2: Verify daemon against live notification database</name>
  <files>nchook.py</files>
  <action>
Human verifies the complete daemon works against the live macOS notification database. This requires Full Disk Access which cannot be granted programmatically.

**What was built:** Complete notification engine daemon (nchook.py) that watches the macOS notification center database via kqueue on the WAL file, extracts notification fields including subtitle/chat name from binary plist blobs, persists read position to state.json via atomic writes, detects DB purges, and validates FDA on startup.

**Pre-requisite: Grant Full Disk Access to your terminal app.**
1. Open System Settings > Privacy and Security > Full Disk Access
2. Click the + button and add your terminal app (Terminal.app, iTerm2, Warp, etc.)
3. Restart your terminal

**Test 1: Startup summary**
Run `cd ~/Projects/macos-notification-intercept && python3 nchook.py`
Expected: Prints a startup banner showing DB path, FDA status OK, and last rec_id (0 on first run). Then begins watching.

**Test 2: Notification detection**
With the daemon running, trigger a notification on your Mac (send yourself a Teams message, or have someone message you). Within ~5 seconds, the daemon should log a line showing the notification's app, title, subtitle, body, and timestamp.

**Test 3: State persistence**
1. Let the daemon process at least one notification
2. Press Ctrl+C to stop it
3. Check that state.json exists and contains a last_rec_id value > 0
4. Run python3 nchook.py again
5. The startup summary should show the last rec_id from state.json
6. Old notifications should NOT be re-logged (only new ones)

**Test 4: Fields extracted correctly**
For a Teams notification, verify the logged output includes: app (contains "teams"), title (sender name), subtitle (chat/channel name), body (message content), timestamp (ISO 8601 format).
  </action>
  <verify>
Human confirms: startup summary displays correctly, new notifications are detected within seconds, state persists across restarts, extracted fields are correct.
  </verify>
  <done>
Human has verified the daemon works against the live macOS notification database with all five Phase 1 success criteria met: startup summary, notification detection, state persistence, FDA validation, and correct field extraction.
  </done>
</task>

</tasks>

<verification>
1. `python3 nchook.py` runs without crashes (starts watching or prints FDA error)
2. New notifications are detected within 5 seconds and logged with all fields
3. state.json is created after first notification batch, persists across restarts
4. Restart resumes from last rec_id without replaying old notifications
5. DB purge (if testable) resets state with warning
6. kqueue watches WAL file; fallback poll kicks in on timeout
</verification>

<success_criteria>
- The daemon runs as a foreground process, watching the notification DB in near-real-time
- Each new notification is logged with: app bundle ID, title (sender), subtitle (chat/channel), body (message), timestamp
- State persists across daemon restarts -- no replay of old notifications
- Human has verified the daemon works against the live macOS notification database
</success_criteria>

<output>
After completion, create `.planning/phases/01-db-watcher-state-engine/01-02-SUMMARY.md`
</output>
